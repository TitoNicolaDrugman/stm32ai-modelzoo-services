# Text Generation STM32 Model Zoo

This folder provides a **reference implementation and tooling to train, quantize, evaluate, benchmark and deploy compact text generation (language modeling) networks on STM32 devices** (MCU / MPU) using the STM32Cube.AI toolchain.

> Focus: lightweight Transformer‚Äêstyle (e.g. *tiny BERT generator*) word level language models suitable for on‚Äëdevice next‚Äëtoken generation.

---

## Directory components

* **`datasets/`** ‚Äì Placeholder + utilities (`combine_corpus.py`) for assembling training / validation corpora. Contains example public domain texts (Shakespeare, Moby Dick, etc.) and a `README.md` / `Source.md` describing provenance.
* **`src/`** ‚Äì Core Python packages implementing training, evaluation, prediction (generation), quantization, preprocessing, benchmarking, utilities, models, and configuration examples.

  * `models/` ‚Äì Model definitions (e.g. `tiny_bert_generator.py`).
  * `preprocessing/` ‚Äì Data loading / tokenization & corpus preparation helpers.
  * `training/` ‚Äì Training loop, callbacks, learning rate scheduling, augmentation (if any), and documentation.
  * `evaluation/` ‚Äì Accuracy / perplexity / quality evaluation utilities.
  * `prediction/` ‚Äì Text generation (greedy / sampling) scripts.
  * `quantization/` ‚Äì Post‚Äëtraining quantization & representative dataset handling.
  * `benchmarking/` ‚Äì Host‚Äëside scripts to benchmark runtime, memory, throughput on STM32 targets.
  * `config_file_examples/` ‚Äì Minimal YAML examples for each operation mode.
  * `experiments_outputs/` ‚Äì Auto‚Äëgenerated run artifacts (Hydra configs, logs, TensorBoard, quantized & float models). **Not intended for manual editing.**
  * `utils/` ‚Äì Auxiliary functions (model management, config parsing, header generation, etc.).
* **`deployment/`** ‚Äì Scripts + generated C sources (`c_project/Middlewares/ST/AI/*`) to package a quantized model for STM32 boards (MCU or MPU variants). Includes board‚Äëspecific READMEs (`README_STM32H7.md`, `README_STM32N6.md`, `README_MPU.md`).
* **`conversion_scripts/`** ‚Äì Helper(s) to convert exported `.tflite` or `.h5` artifacts into C arrays / headers.
* **`c_headers/`** ‚Äì Generated headers (e.g. `model.h`, `tokenizer.h`, `embedding.h`) for embedding inside firmware projects.
* **`pretrained_models/`** ‚Äì Collection (or pointers) to example pre‚Äëquantized / quantized language models (e.g. MobileNet reference left here as template ‚Äî replace with text models).
* **`mlruns/`** ‚Äì MLflow tracking directory (local) storing experiment metadata, parameters, metrics, artifacts. Each run subfolder contains logs, TensorBoard events, quantized & float checkpoints.
* **`st_ai_ws/`** ‚Äì (Optional) Workspace integration placeholder for STM32 AI ecosystem tools.
* **Top‚Äëlevel scripts** ‚Äì `stm32ai_main.py` (entrypoint orchestrating chains), `check_tflite*.py` sanity checks, evaluation helpers, `user_config.yaml` exemplar configuration.

---

## Core Concepts

The workflow is orchestrated by `stm32ai_main.py` using an `operation_mode` value from the YAML configuration. Modes encapsulate single services (training, quantization, evaluation, prediction, benchmarking, deployment) or *chains* that sequence them.

**Typical metrics:** loss (cross‚Äëentropy), accuracy (if framed as next‚Äëtoken classification), validation loss, and derived perplexity (can be computed as `exp(loss)`).

---

## Operation Modes

In chain names: **t** = training, **q** = quantization, **e** = evaluation, **b** = benchmarking, **d** = deployment.

| `operation_mode` | Operations (in order)                                                                    | Notes                                                   |
| :--------------- | :--------------------------------------------------------------------------------------- | :------------------------------------------------------ |
| `training`       | Train a text generation model (BYOD ‚Äì bring your own dataset / BYOM ‚Äì your architecture) | Produces float model + checkpoints                      |
| `evaluation`     | Evaluate a float or quantized model                                                      | Computes metrics (loss / accuracy / perplexity)         |
| `quantization`   | Quantize a float model                                                                   | Creates `.tflite` + representative data usage           |
| `prediction`     | Generate text using a float or quantized model                                           | Supports greedy / temperature sampling (if implemented) |
| `benchmarking`   | Benchmark model on STM32 board                                                           | Measures latency / memory                               |
| `deployment`     | Package & deploy model on STM32 board                                                    | Generates C project in `deployment/c_project`           |
| `chain_tqe`      | training ‚Üí quantization ‚Üí evaluation                                                     | Fast end‚Äëto‚Äëend quality check                           |
| `chain_tqeb`     | training ‚Üí quantization ‚Üí evaluation ‚Üí benchmarking                                      | Adds on‚Äëdevice performance profile                      |
| `chain_eqe`      | evaluation (float) ‚Üí quantization ‚Üí evaluation (quantized)                               | Compare float vs quantized metrics                      |
| `chain_eqeb`     | evaluation (float) ‚Üí quantization ‚Üí evaluation (quantized) ‚Üí benchmarking                | Quality + performance delta                             |
| `chain_qb`       | quantization ‚Üí benchmarking                                                              | For existing float model                                |
| `chain_qd`       | quantization ‚Üí deployment                                                                | Direct field deployment                                 |

---

## Model Types

Initial reference model: **`tiny_bert_generator`** (compact Transformer encoder producing next‚Äëtoken logits). Additional model variants can be added under `src/models/` ‚Äì ensure each exposes a factory function and is referenced in configuration.

*Add more lightweight architectures (e.g. distilled transformer, GRU / LSTM baselines) for comparative benchmarking.*

---

## Datasets & Tokenization

Place raw corpus files (plain text, UTF‚Äë8) inside `datasets/`. Use `combine_corpus.py` to merge or preprocess multiple sources into `full_corpus.txt`.

Tokenization strategy (example): simple whitespace / word index (see `tokenizer.json`). You may substitute a character‚Äëlevel or subword tokenizer; update preprocessing + config accordingly.

**Representative data for quantization**: by default pulled from the training corpus unless a dedicated quantization path is specified.

---

## Configuration Files

All tunable settings reside in YAML (Hydra/OmegaConf). Main sections typically include:

* `general`: run naming, model path, output directory.
* `dataset`: paths, sequence length, vocabulary specs, batch sizes.
* `model`: architecture hyperparameters (embedding dim, heads, layers, FFN size, etc.).
* `training`: epochs, optimizer params, learning rate schedule, early stopping.
* `quantization`: representative dataset size, calibration parameters.
* `prediction`: prompt, max tokens, decoding temperature / top‚Äëk (if available).
* `deployment` & `benchmarking`: board type, clock config, runtime profiling options.
* `tools.stm32ai`: on‚Äëcloud / local settings, version.

Override any parameter from the CLI (Hydra pattern): `section.param=value`.

---

## Typical Workflows

**1. Train & Evaluate**

```
operation_mode=training
```

Inspect metrics (TensorBoard) in `src/experiments_outputs/<run>/logs`.

**2. Quantize & Compare**

```
operation_mode=chain_eqe model.float_model_path=<path_to_h5>
```

Check metric deltas (accuracy / loss / perplexity) pre vs post quantization.

**3. Benchmark On Device**

```
operation_mode=chain_qb deployment.hardware_setup.board=STM32H7
```

Review latency / memory usage reported by benchmarking scripts.

**4. Deploy**

```
operation_mode=chain_qd deployment.hardware_setup.board=STM32N6570-DK
```

Generated C sources land under `deployment/c_project/Middlewares/ST/AI/`.

**5. Generate Text**

```
operation_mode=prediction prediction.prompt="STM32 embedded AI" prediction.max_tokens=50
```

Retrieve generated sequence from console / saved logs.

---

## Benchmarking & MLflow

Each run logs parameters + metrics to `mlruns/` (MLflow). TensorBoard events stored in `.../logs/`. Use these to compare architectures or quantization impacts across runs.

---

## Deployment

Deployment scripts integrate with STM32Cube.AI (local or Developer Cloud if `tools.stm32ai.on_cloud=true`). After `chain_qd`, follow board README instructions to flash and (if needed) toggle boot switches (e.g. STM32N6570-DK notice).

Generated headers (`model.h`, `tokenizer.h`, `embedding.h`) are prepared for inclusion in application firmware.

---

## Extending the Zoo

1. Add new model file under `src/models/`.
2. Expose a build function returning a compiled `tf.keras.Model`.
3. Register / reference it in configuration (`model.name` or similar parameter).
4. Provide a minimal config example for training & quantization.
5. (Optional) Supply a pre‚Äëtrained & quantized artifact in `pretrained_models/`.

---

## License

See `LICENSE.md` and any embedded license files inside generated middleware folders.

---

## Getting Help

Check individual READMEs inside subfolders for mode‚Äëspecific guidance (training, quantization, evaluation, deployment). For quick orientation, start with a small corpus + reduced model dimensions, then scale once the pipeline works end‚Äëto‚Äëend.

Happy generating on STM32! üöÄ
