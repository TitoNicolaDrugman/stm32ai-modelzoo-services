general:
  project_name: tinybert_word_generation
  logs_dir: logs
  saved_models_dir: saved_models
  global_seed: 42
mlflow:
  uri: ./mlruns
  experiment_name: tinybert_word_generation
operation_mode: chain_tqe
dataset:
  name: tinyshakespeare
  corpus_path: ./datasets/full_corpus.txt
  sequence_length: 30
  max_vocab_size: 20000
  validation_split: 0.1
  test_split: 0.1
  subset_size: 0
preprocessing:
  tokenizer_path: ./src/experiments_outputs/tokenizer.json
training:
  model:
    name: tiny_bert_generator
    vocab_size: 0
    embedding_dim: 128
    num_layers: 6
    num_heads: 4
    dff: 1024
    input_shape:
    - 30
    include_embedding: true
  batch_size: 128
  epochs: 100
  optimizer:
    Adam:
      learning_rate: 0.001
  callbacks:
    ReduceLROnPlateau:
      monitor: val_loss
      mode: min
      factor: 0.5
      patience: 30000000
      min_lr: 1.0e-05
    EarlyStopping:
      monitor: val_loss
      mode: min
      restore_best_weights: true
      patience: 5
quantization:
  quantizer: TFlite_converter
  quantization_type: PTQ
  quantization_input_type: int8
  quantization_output_type: int8
  export_dir: quantized_models
  model:
    include_embedding: false
evaluation:
  sample_text: To be or not to be
  gen_length: 20
  temperature: 1.2
