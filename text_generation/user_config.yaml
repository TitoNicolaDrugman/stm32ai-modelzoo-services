# user_config.yaml – word-level Tiny‑BERT project

# ------------------------------------------------------
# General settings
# ------------------------------------------------------
general:
  project_name: tinybert_word_generation
  logs_dir: logs
  saved_models_dir: saved_models
  global_seed: 42

# ------------------------------------------------------
# Where Hydra stores each run – we replicate the old structure:
# src/experiments_outputs/<YYYY_MM_DD_HH_MM_SS>
# ------------------------------------------------------
hydra:
  run:
    dir: ./src/experiments_outputs/${now:%Y_%m_%d_%H_%M_%S}

# ------------------------------------------------------
# MLflow tracking
# ------------------------------------------------------
mlflow:
  uri: "./mlruns"
  experiment_name: tinybert_word_generation

# ------------------------------------------------------
# Pipeline mode
# ------------------------------------------------------
operation_mode: chain_tqe

# ------------------------------------------------------
# Dataset
# ------------------------------------------------------
dataset:
  name: tinyshakespeare
  corpus_path: ./datasets/full_corpus.txt #tinyshakespeare.txt
  sequence_length: 30
  max_vocab_size: 20000 #5000
  validation_split: 0.1
  test_split: 0.1
  subset_size: 0

# ------------------------------------------------------
# Preprocessing
# ------------------------------------------------------
preprocessing:
  tokenizer_path: ./src/experiments_outputs/tokenizer.json

# ------------------------------------------------------
# Training
# ------------------------------------------------------
training:
  model:
    name: tiny_bert_generator
    vocab_size: 0
    embedding_dim: 128
    num_layers: 6 #4
    num_heads: 4
    dff: 1024 #512
    input_shape: [30]
    include_embedding: true
  batch_size: 128
  epochs: 100 #50
  optimizer:
    Adam:
      learning_rate: 0.001
  callbacks:
    ReduceLROnPlateau:
      monitor: val_loss
      mode: min
      factor: 0.5
      patience: 3 #by default 3
      min_lr: 1.0e-5
    EarlyStopping:
      monitor: val_loss
      mode: min
      restore_best_weights: true
      patience: 5

# ------------------------------------------------------
# Quantization
# ------------------------------------------------------
quantization:
  quantizer: TFlite_converter
  quantization_type: PTQ
  quantization_input_type: int8
  quantization_output_type: int8
  export_dir: quantized_models
  model:
    include_embedding: false

# ------------------------------------------------------
# Evaluation
# ------------------------------------------------------
evaluation:
  sample_text: "To be or not to be"
  gen_length: 20
  temperature: 1.2
